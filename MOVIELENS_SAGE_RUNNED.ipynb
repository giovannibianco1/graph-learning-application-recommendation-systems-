{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ptk1J307IVn8"
      },
      "source": [
        "MovieLens Rating Prediction Notebook\n",
        "\n",
        "This notebook runs faster on a GPU runtime. To enable it, go to Edit > Notebook Settings > Hardware Accelerator > GPU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqYLHVWoIgSe"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKXgOkl6Iabv",
        "outputId": "0bd284fc-fd06-4f17-d1a9-658c9e99e7c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.4.0+cu121\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(torch.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pV0Rkm6tIqcB",
        "outputId": "42823aae-ca3c-4538-df10-3f93b04b8f2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/nightly/torch-2.4.0+cu121.html\n",
            "Requirement already satisfied: pyg-lib in /usr/local/lib/python3.10/dist-packages (0.4.0.dev20240909+pt24cu121)\n",
            "Collecting git+https://github.com/pyg-team/pytorch_geometric.git\n",
            "  Cloning https://github.com/pyg-team/pytorch_geometric.git to /tmp/pip-req-build-arokfgn6\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/pyg-team/pytorch_geometric.git /tmp/pip-req-build-arokfgn6\n",
            "  Resolved https://github.com/pyg-team/pytorch_geometric.git to commit 241a8c3d018636c116fd1fd7fa2ab9ff3925531e\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.6.0) (3.10.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.6.0) (2024.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.6.0) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.6.0) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.6.0) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.6.0) (3.1.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.6.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.6.0) (4.66.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric==2.6.0) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric==2.6.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric==2.6.0) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric==2.6.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric==2.6.0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric==2.6.0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric==2.6.0) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric==2.6.0) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric==2.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric==2.6.0) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric==2.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric==2.6.0) (2024.8.30)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.4.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.24.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: fuzzywuzzy[speedup] in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: python-levenshtein>=0.12 in /usr/local/lib/python3.10/dist-packages (from fuzzywuzzy[speedup]) (0.25.1)\n",
            "Requirement already satisfied: Levenshtein==0.25.1 in /usr/local/lib/python3.10/dist-packages (from python-levenshtein>=0.12->fuzzywuzzy[speedup]) (0.25.1)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from Levenshtein==0.25.1->python-levenshtein>=0.12->fuzzywuzzy[speedup]) (3.9.7)\n",
            "Requirement already satisfied: captum in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from captum) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from captum) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from captum) (2.4.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from captum) (4.66.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (2024.6.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->captum) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6->captum) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "import os\n",
        "\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-${TORCH}.html\n",
        "!pip install pyg-lib -f https://data.pyg.org/whl/nightly/torch-${TORCH}.html\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "\n",
        "!pip install sentence_transformers\n",
        "!pip3 install fuzzywuzzy[speedup]\n",
        "!pip install captum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WJ8piSnIy2f"
      },
      "source": [
        "## Link Regression on the MovieLens Dataset\n",
        "\n",
        "This notebook shows how to load a set of `*.csv` files into a `torch_geometric.data.HeteroData` object and how to train a [heterogeneous graph model](https://pytorch-geometric.readthedocs.io/en/latest/notes/heterogeneous.html#hgtutorial).\n",
        "\n",
        "We are going to use the [Movielens dataset](https://grouplens.org/datasets/movielens/), which is collected by the GroupLens Research group. The toy dataset describes movies, users, and their ratings. We are going to predict the rating of a user for a movie."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51OK1cQL9V9e"
      },
      "source": [
        "## Data Ingestion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjJrBa3J0btD",
        "outputId": "e4690c65-14f5-4c20-d83a-de34c13ba333"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using existing file ml-latest-small.zip\n",
            "Extracting ./ml-latest-small.zip\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.data import download_url, extract_zip\n",
        "import pandas as pd\n",
        "\n",
        "dataset_name = 'ml-latest-small'\n",
        "\n",
        "url = f'https://files.grouplens.org/datasets/movielens/{dataset_name}.zip'\n",
        "extract_zip(download_url(url, '.'), '.')\n",
        "\n",
        "movies_path = f'./{dataset_name}/movies.csv'\n",
        "ratings_path = f'./{dataset_name}/ratings.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haJz-BYBI2wi",
        "outputId": "576c2efa-fdad-4d54-cb2f-f85084cb88bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "movies.csv:\n",
            "===========\n",
            "                                              genres  \\\n",
            "movieId                                                \n",
            "1        Adventure|Animation|Children|Comedy|Fantasy   \n",
            "2                         Adventure|Children|Fantasy   \n",
            "3                                     Comedy|Romance   \n",
            "4                               Comedy|Drama|Romance   \n",
            "5                                             Comedy   \n",
            "\n",
            "                                      title  \n",
            "movieId                                      \n",
            "1                          Toy Story (1995)  \n",
            "2                            Jumanji (1995)  \n",
            "3                   Grumpier Old Men (1995)  \n",
            "4                  Waiting to Exhale (1995)  \n",
            "5        Father of the Bride Part II (1995)  \n",
            "Number of movies: 9742\n",
            "\n",
            "ratings.csv:\n",
            "============\n",
            "   userId  movieId  rating\n",
            "0       1        1     4.0\n",
            "1       1        3     4.0\n",
            "2       1        6     4.0\n",
            "3       1       47     5.0\n",
            "4       1       50     5.0\n",
            "Number of ratings: 100836\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load the entire ratings dataframe into memory:\n",
        "ratings_df = pd.read_csv(ratings_path)[[\"userId\", \"movieId\", \"rating\"]]\n",
        "\n",
        "# Load the entire movie dataframe into memory:\n",
        "movies_df = pd.read_csv(movies_path, index_col='movieId')\n",
        "\n",
        "print('movies.csv:')\n",
        "print('===========')\n",
        "print(movies_df[[\"genres\", \"title\"]].head())\n",
        "print(f\"Number of movies: {len(movies_df)}\")\n",
        "print()\n",
        "print('ratings.csv:')\n",
        "print('============')\n",
        "print(ratings_df[[\"userId\", \"movieId\", \"rating\"]].head())\n",
        "print(f\"Number of ratings: {len(ratings_df)}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK1yWr3uBjaS"
      },
      "source": [
        "Additionally, let's add our ratings to the dataset to get predictions for movies we haven't seen yet.\n",
        "\n",
        "There are two ways to add ratings:\n",
        "1. **Add ratings manually**\n",
        "2. **Upload IMDB ratings**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOA1_3IGBqCQ"
      },
      "source": [
        "### Add your ratings manually\n",
        "\n",
        "\n",
        "We recommend adding at least 10 ratings. Let's first check out the most rated movies. Additional movies in the table are: *Avatar*, *The Dark Knight*, *Pretty Women*,\n",
        "*Titanic*, *The Lion King*, *Jurassic Park*, *The Matrix*, *The Lord of the Rings* and *The Avengers*. Please note that the article in the movie title is often at the end of the title."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5rGmyzUBoW7",
        "outputId": "0eb3657f-0a9d-466f-9449-71707f381e46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most rated movies:\n",
            "==================\n",
            "                                             title\n",
            "movieId                                           \n",
            "356                            Forrest Gump (1994)\n",
            "318               Shawshank Redemption, The (1994)\n",
            "296                            Pulp Fiction (1994)\n",
            "593               Silence of the Lambs, The (1991)\n",
            "2571                            Matrix, The (1999)\n",
            "260      Star Wars: Episode IV - A New Hope (1977)\n",
            "480                           Jurassic Park (1993)\n",
            "110                              Braveheart (1995)\n",
            "589              Terminator 2: Judgment Day (1991)\n",
            "527                        Schindler's List (1993)\n"
          ]
        }
      ],
      "source": [
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "# Specify your userId\n",
        "our_user_id = ratings_df['userId'].max() + 1\n",
        "\n",
        "print('Most rated movies:')\n",
        "print('==================')\n",
        "most_rated_movies = ratings_df['movieId'].value_counts().head(10)\n",
        "print(movies_df.loc[most_rated_movies.index][[\"title\"]])\n",
        "\n",
        "# Initialize your rating list\n",
        "ratings = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llcJK-CSB2q0",
        "outputId": "6376fe30-ab24-4711-e881-0a0c87d9c550"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Select the 1. movie:\n",
            "=====================================\n",
            "Please enter the movie title: eternal sunshine of the spotless mind\n",
            "                                                title\n",
            "movieId                                              \n",
            "7361     Eternal Sunshine of the Spotless Mind (2004)\n",
            "95558              Beasts of the Southern Wild (2012)\n",
            "2530            Beneath the Planet of the Apes (1970)\n",
            "7318                Passion of the Christ, The (2004)\n",
            "2531         Battle for the Planet of the Apes (1973)\n",
            "Please enter the movie id: 7361\n",
            "Please enter your rating: 4.8\n",
            "\n",
            "Select the 2. movie:\n",
            "=====================================\n",
            "Please enter the movie title: the shining\n",
            "                           title\n",
            "movieId                         \n",
            "180263        The Shining (1997)\n",
            "165635       The Thinning (2016)\n",
            "159061        The Wailing (2016)\n",
            "4591      Erik the Viking (1989)\n",
            "7831     Another Thin Man (1939)\n",
            "Please enter the movie id: 180263\n",
            "Please enter your rating: 4.4\n",
            "\n",
            "Select the 3. movie:\n",
            "=====================================\n",
            "Please enter the movie title: american psycho\n",
            "                            title\n",
            "movieId                          \n",
            "3535       American Psycho (2000)\n",
            "53                Lamerica (1994)\n",
            "3725          American Pop (1981)\n",
            "52299    American Hardcore (2006)\n",
            "80126        American, The (2010)\n",
            "Please enter the movie id: 3535\n",
            "Please enter your rating: 4.3\n",
            "\n",
            "Select the 4. movie:\n",
            "=====================================\n",
            "Please enter the movie title: shutter island\n",
            "                           title\n",
            "movieId                         \n",
            "74458      Shutter Island (2010)\n",
            "15       Cutthroat Island (1995)\n",
            "73386       Staten Island (2009)\n",
            "63515          The Island (2006)\n",
            "95717     Treasure Island (2012)\n",
            "Please enter the movie id: 74458\n",
            "Please enter your rating: 4.5\n",
            "\n",
            "Select the 5. movie:\n",
            "=====================================\n",
            "Please enter the movie title: in time\n",
            "                            title\n",
            "movieId                          \n",
            "7055            Swing Time (1936)\n",
            "90405              In Time (2011)\n",
            "170993   Mini's First Time (2006)\n",
            "73822             Meantime (1984)\n",
            "31223       Racing Stripes (2005)\n",
            "Please enter the movie id: 90405\n",
            "Please enter your rating: 3.8\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Add your ratings here:\n",
        "num_ratings = 5\n",
        "\n",
        "while len(ratings) < num_ratings:\n",
        "    print(f'Select the {len(ratings) + 1}. movie:')\n",
        "    print('=====================================')\n",
        "    movie = input('Please enter the movie title: ')\n",
        "    movies_df['title_score'] = movies_df['title'].apply(lambda x: fuzz.ratio(x, movie))\n",
        "    print(movies_df.sort_values('title_score', ascending=False)[['title']].head(5))\n",
        "    movie_id = input('Please enter the movie id: ')\n",
        "    if not movie_id:\n",
        "        continue\n",
        "    movie_id = int(movie_id)\n",
        "    rating = float(input('Please enter your rating: '))\n",
        "    if not rating:\n",
        "        continue\n",
        "    assert 0 <= rating <= 5\n",
        "    ratings.append({'movieId': movie_id, 'rating': rating, 'userId': our_user_id})\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCR0uLeGB_xa"
      },
      "outputs": [],
      "source": [
        "# Add your ratings to the rating dataframe\n",
        "ratings_df = pd.concat([ratings_df, pd.DataFrame.from_records(ratings)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vy4EpbbGCCP9"
      },
      "source": [
        "### Upload your IMDB ratings (Optional)\n",
        "\n",
        "If you have an IMDB account, you can also upload your IMDB ratings. To do so, please follow the following steps:\n",
        "1. Go to https://www.imdb.com/\n",
        "2. Login to your account\n",
        "3. Go to `Your Ratings`\n",
        "4. Click on `Export Ratings` after clicking the three dots in the upper right corner\n",
        "5. Upload the downloaded `ratings.csv` file to the current directory\n",
        "6. Rename the file to `imdb_ratings.csv`\n",
        "7. Run the following cell\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6x3ZJgHRB0Jm"
      },
      "outputs": [],
      "source": [
        "# Select our userId\n",
        "our_user_id = ratings_df['userId'].max() + 1\n",
        "\n",
        "# Load the IMDB ratings:\n",
        "imdb_rating_path = f'./imdb_ratings.csv'\n",
        "imdb_ratings_df = pd.read_csv(imdb_rating_path)\n",
        "imdb_ratings_df.columns = imdb_ratings_df.columns.str.strip().str.lower()\n",
        "\n",
        "# The IMDB movie titles / ids do not match the movie titles /ids in the movielens dataframes\n",
        "# so we need to map them:\n",
        "imdb_ratings_df['title'] = imdb_ratings_df['title'] + ' (' + imdb_ratings_df['year'].astype(str) + ')'\n",
        "imdb_ratings_df['title'] = imdb_ratings_df['title'].str.strip()\n",
        "movies_df['title'] = movies_df['title'].str.strip()\n",
        "imdb_ratings_df = imdb_ratings_df.merge(movies_df['title'].reset_index(), on='title', how='inner', )\n",
        "\n",
        "# The ratings are on a scale from 1 to 10, so we need to transform them to a scale from 0 to 5:\n",
        "imdb_ratings_df['rating'] = (imdb_ratings_df['your rating'] / 2).astype(int)\n",
        "\n",
        "# Your ratings that we are going to use:\n",
        "print('Your IMDB ratings:')\n",
        "print('==================')\n",
        "print(imdb_ratings_df[['title', 'rating']].head(10))\n",
        "\n",
        "# Finally, we can add the ratings to the ratings data frame:\n",
        "imdb_ratings_df['userId'] = our_user_id\n",
        "ratings_df = pd.concat([ratings_df, imdb_ratings_df[['movieId', 'rating', 'userId']]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uhaQNVsI76a"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "We are going to use the genre as well as the title of the movie as node features. For the `title` features, we are going to use a pre-trained [sentence transformer](https://www.sbert.net/) model to encode the title into a vector.\n",
        "For the `genre` features, we are going to use a one-hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239,
          "referenced_widgets": [
            "f832a6504b7d4797b2f76aea083c7d30",
            "cc388e3418e747a9861c5d5d93514d75",
            "a9e6e732503c462ea4e22a88235a0542",
            "548f0ec5c73b41f09b524eeeae123990",
            "70ffd13684be4cea9306fadaebb743f5",
            "73fec9bd6c4645a5a78bd4d27cc171f3",
            "05f7c31612a341018fb0812105d2b37c",
            "831622595932480fba9d6e7a58dce41b",
            "670f5c889f9748c7b2a37cfecc674022",
            "716875445e814266aa06d408d6e7edc2",
            "7ac4ce33a1f74874997ec89cd0723b16"
          ]
        },
        "id": "fXF-BNIYJAMo",
        "outputId": "fd6617a1-a41b-4d70-cc91-70b52055dc1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/305 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f832a6504b7d4797b2f76aea083c7d30"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# One-hot encode the genres:\n",
        "genres = movies_df['genres'].str.get_dummies('|').values\n",
        "genres = torch.from_numpy(genres).to(torch.float)\n",
        "\n",
        "# Load the pre-trained sentence transformer model and encode the movie titles:\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "with torch.no_grad():\n",
        "    titles = model.encode(movies_df['title'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
        "    titles = titles.cpu()\n",
        "\n",
        "# Concatenate the genres and title features:\n",
        "movie_features = torch.cat([genres, titles], dim=-1)\n",
        "\n",
        "# We don't have user features, which is why we use an identity matrix\n",
        "user_features = torch.eye(len(ratings_df['userId'].unique()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cHyIhDgJCZa"
      },
      "source": [
        "The `ratings.csv` file contains the ratings of users for movies. From this\n",
        "file we are extracting the `userId`. We create a mapping from the `userId`\n",
        "to a unique consecutive value in the range `[0, num_users]`. This is needed as we want our final data representation to be as compact as possible, *e.g.*, the representation of a user in the first row should be accessible via `x[0]`.\n",
        "The same we do for the `movieId`.\n",
        "Afterwards, we obtain the final `edge_index` representation of shape `[2, num_ratings]` from `ratings.csv` by merging mapped user and movie indices with the raw indices given by the original data frame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7YZJLJVJEbL",
        "outputId": "471ea725-4e00-4bab-f757-b47e28e80955"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mapping of user IDs to consecutive values:\n",
            "==========================================\n",
            "   userId  mappedUserId\n",
            "0       1             0\n",
            "1       2             1\n",
            "2       3             2\n",
            "3       4             3\n",
            "4       5             4\n",
            "\n",
            "Mapping of movie IDs to consecutive values:\n",
            "===========================================\n",
            "   movieId  mappedMovieId\n",
            "0        1              0\n",
            "1        3              1\n",
            "2        6              2\n",
            "3       47              3\n",
            "4       50              4\n",
            "\n",
            "Final edge indices pointing from users to movies:\n",
            "================================================\n",
            "tensor([[ 0,  4,  6, 14, 16, 17, 18, 20, 26, 30],\n",
            "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Create a mapping from the userId to a unique consecutive value in the range [0, num_users]:\n",
        "unique_user_id = ratings_df['userId'].unique()\n",
        "unique_user_id = pd.DataFrame(data={\n",
        "    'userId': unique_user_id,\n",
        "    'mappedUserId': pd.RangeIndex(len(unique_user_id))\n",
        "    })\n",
        "print(\"Mapping of user IDs to consecutive values:\")\n",
        "print(\"==========================================\")\n",
        "print(unique_user_id.head())\n",
        "print()\n",
        "\n",
        "# Create a mapping from the movieId to a unique consecutive value in the range [0, num_movies]:\n",
        "unique_movie_id = ratings_df['movieId'].unique()\n",
        "unique_movie_id = pd.DataFrame(data={\n",
        "    'movieId': unique_movie_id,\n",
        "    'mappedMovieId': pd.RangeIndex(len(unique_movie_id))\n",
        "    })\n",
        "print(\"Mapping of movie IDs to consecutive values:\")\n",
        "print(\"===========================================\")\n",
        "print(unique_movie_id.head())\n",
        "print()\n",
        "\n",
        "# Merge the mappings with the original data frame:\n",
        "ratings_df = ratings_df.merge(unique_user_id, on='userId')\n",
        "ratings_df = ratings_df.merge(unique_movie_id, on='movieId')\n",
        "\n",
        "# With this, we are ready to create the edge_index representation in COO format\n",
        "# following the PyTorch Geometric semantics:\n",
        "edge_index = torch.stack([\n",
        "    torch.tensor(ratings_df['mappedUserId'].values),\n",
        "    torch.tensor(ratings_df['mappedMovieId'].values)]\n",
        "    , dim=0)\n",
        "\n",
        "assert edge_index.shape == (2, len(ratings_df))\n",
        "\n",
        "print(\"Final edge indices pointing from users to movies:\")\n",
        "print(\"================================================\")\n",
        "print(edge_index[:, :10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfnycIvfJHrx"
      },
      "source": [
        "## Heterogeneous Graph Construction\n",
        "\n",
        "With this we are ready to initialize our heterogeneous graph data object and pass the\n",
        "necessary information to it.\n",
        "\n",
        "We also take care of adding reverse edges to the `HeteroData` object. This allows our GNN\n",
        "model to use both directions of the edges for the message passing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDQaX8OPJMvj",
        "outputId": "189b6ab5-14a8-4d74-ae37-4b622a7b8716"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HeteroData(\n",
              "  user={ x=[611, 611] },\n",
              "  movie={ x=[9742, 404] },\n",
              "  (user, rates, movie)={\n",
              "    edge_index=[2, 100841],\n",
              "    edge_label=[100841],\n",
              "  },\n",
              "  (movie, rev_rates, user)={ edge_index=[2, 100841] }\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.data import HeteroData\n",
        "\n",
        "# Create the heterogeneous graph data object:\n",
        "data = HeteroData()\n",
        "\n",
        "# Add the user nodes:\n",
        "data['user'].x = user_features  # [num_users, num_features_users]\n",
        "\n",
        "# Add the movie nodes:\n",
        "data['movie'].x = movie_features  # [num_movies, num_features_movies]\n",
        "\n",
        "# Add the rating edges:\n",
        "data['user', 'rates', 'movie'].edge_index = edge_index  # [2, num_ratings]\n",
        "\n",
        "# Add the rating labels:\n",
        "rating = torch.from_numpy(ratings_df['rating'].values).to(torch.float)\n",
        "data['user', 'rates', 'movie'].edge_label = rating  # [num_ratings]\n",
        "\n",
        "# We also need to make sure to add the reverse edges from movies to users\n",
        "# in order to let a GNN be able to pass messages in both directions.\n",
        "# We can leverage the `T.ToUndirected()` transform for this from PyG:\n",
        "data = T.ToUndirected()(data)\n",
        "\n",
        "# With the above transformation we also got reversed labels for the edges.\n",
        "# We are going to remove them:\n",
        "del data['movie', 'rev_rates', 'user'].edge_label\n",
        "\n",
        "assert data['user'].num_nodes == len(unique_user_id)\n",
        "assert data['user', 'rates', 'movie'].num_edges == len(ratings_df)\n",
        "assert data['movie'].num_features == 404\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpcH0bniJaJw"
      },
      "source": [
        "## Dataset Splitting\n",
        "\n",
        "We can now split our data into a training, validation and test set. We are going to use\n",
        "the `T.RandomLinkSplit` transform from PyG to do this. This transform will randomly\n",
        "split the links with their label/rating into training, validation and test set.\n",
        "We are going to use 80% of the edges for training, 10% for validation and 10% for testing.\n",
        "\n",
        "<font color='red'>Please note this part is implemented for you and do not modify it for the grading purpose. However, feel free to use different configurations just for fun</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njo191z3JctD",
        "outputId": "a33dce9b-3307-49dc-994c-bf88a5aff41c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(HeteroData(\n",
              "   user={ x=[611, 611] },\n",
              "   movie={ x=[9742, 404] },\n",
              "   (user, rates, movie)={\n",
              "     edge_index=[2, 80673],\n",
              "     edge_label=[80673],\n",
              "     edge_label_index=[2, 80673],\n",
              "   },\n",
              "   (movie, rev_rates, user)={ edge_index=[2, 80673] }\n",
              " ),\n",
              " HeteroData(\n",
              "   user={ x=[611, 611] },\n",
              "   movie={ x=[9742, 404] },\n",
              "   (user, rates, movie)={\n",
              "     edge_index=[2, 80673],\n",
              "     edge_label=[10084],\n",
              "     edge_label_index=[2, 10084],\n",
              "   },\n",
              "   (movie, rev_rates, user)={ edge_index=[2, 80673] }\n",
              " ))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "train_data, val_data, test_data = T.RandomLinkSplit(\n",
        "    num_val=0.1,\n",
        "    num_test=0.1,\n",
        "    neg_sampling_ratio=0.0,\n",
        "    edge_types=[('user', 'rates', 'movie')],\n",
        "    rev_edge_types=[('movie', 'rev_rates', 'user')],\n",
        ")(data)\n",
        "train_data, val_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfg4wVcNJfFY"
      },
      "source": [
        "## Graph Neural Network\n",
        "\n",
        "The following model is provided for you, and your own GraphSage and GAT layers will serve as components during the model training process. You need to implement the model below yourself, which will be used for future training.\n",
        "\n",
        "<font color='red'>This is optional and will add bonus points to your project.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WitlFpu7CDnP"
      },
      "source": [
        "## Graph neural network (GNN) model\n",
        "We are now ready to define our GNN model. We are going to use a simple GNN model with\n",
        "two message passing layers for the encoding of the user and movie nodes.\n",
        "Additionally, we are going to use a decoder to predict the rating for the encoded\n",
        "user-movie combination.\n",
        "\n",
        "<font color='red'>You may choose your own GNN layers implemented above, or you can use the provided GNN layers, such as GAT, GCN, etc., from the PyTorch Geometric library</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fl5W1gg5Jhzz",
        "outputId": "1a51db3e-97a1-45ff-a334-e620686985f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(\n",
            "  (encoder): GraphModule(\n",
            "    (conv1): ModuleDict(\n",
            "      (user__rates__movie): SAGEConv((-1, -1), 32, aggr=mean)\n",
            "      (movie__rev_rates__user): SAGEConv((-1, -1), 32, aggr=mean)\n",
            "    )\n",
            "    (conv2): ModuleDict(\n",
            "      (user__rates__movie): SAGEConv((-1, -1), 32, aggr=mean)\n",
            "      (movie__rev_rates__user): SAGEConv((-1, -1), 32, aggr=mean)\n",
            "    )\n",
            "  )\n",
            "  (decoder): EdgeDecoder(\n",
            "    (lin1): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (lin2): Linear(in_features=32, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.nn import SAGEConv, to_hetero, GCNConv\n",
        "\n",
        "class GNNEncoder(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
        "        self.conv2 = SAGEConv((-1, -1), out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "class EdgeDecoder(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super().__init__()\n",
        "        self.lin1 = torch.nn.Linear(2 * hidden_channels, hidden_channels)\n",
        "        self.lin2 = torch.nn.Linear(hidden_channels, 1)\n",
        "\n",
        "    def forward(self, z_dict, edge_label_index):\n",
        "        row, col = edge_label_index\n",
        "        z = torch.cat([z_dict['user'][row], z_dict['movie'][col]], dim=-1)\n",
        "\n",
        "        z = self.lin1(z).relu()\n",
        "        z = self.lin2(z)\n",
        "        return z.view(-1)\n",
        "\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super().__init__()\n",
        "        self.encoder = GNNEncoder(hidden_channels, hidden_channels)\n",
        "        self.encoder = to_hetero(self.encoder, data.metadata(), aggr='sum')\n",
        "        self.decoder = EdgeDecoder(hidden_channels)\n",
        "\n",
        "    def forward(self, x_dict, edge_index_dict, edge_label_index):\n",
        "        z_dict = self.encoder(x_dict, edge_index_dict)\n",
        "        return self.decoder(z_dict, edge_label_index)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = Model(hidden_channels=32).to(device)\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azGW0k2pJoS7"
      },
      "source": [
        "## Training a Heterogeneous GNN\n",
        "\n",
        "Training our GNN is then similar to training any PyTorch model.\n",
        "We move the model to the desired device, and initialize an optimizer that takes care of adjusting model parameters via stochastic gradient descent.\n",
        "\n",
        "The training loop applies the forward computation of the model, computes the loss from ground-truth labels and obtained predictions, and adjusts model parameters via back-propagation and stochastic gradient descent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5_rbeCjJnsz",
        "outputId": "bdc05fbe-817f-4f32-ef56-af417413696f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 13.8394, Train: 3.5297, Val: 3.5489\n",
            "Epoch: 002, Loss: 12.4590, Train: 3.1798, Val: 3.2006\n",
            "Epoch: 003, Loss: 10.1111, Train: 2.5373, Val: 2.5614\n",
            "Epoch: 004, Loss: 6.4381, Train: 1.4820, Val: 1.5074\n",
            "Epoch: 005, Loss: 2.1962, Train: 1.5225, Val: 1.4878\n",
            "Epoch: 006, Loss: 2.3257, Train: 1.8148, Val: 1.7730\n",
            "Epoch: 007, Loss: 4.3919, Train: 1.4343, Val: 1.4028\n",
            "Epoch: 008, Loss: 2.0584, Train: 1.0388, Val: 1.0436\n",
            "Epoch: 009, Loss: 1.0790, Train: 1.3174, Val: 1.3405\n",
            "Epoch: 010, Loss: 1.7356, Train: 1.5622, Val: 1.5875\n",
            "Epoch: 011, Loss: 2.4405, Train: 1.6018, Val: 1.6270\n",
            "Epoch: 012, Loss: 2.5656, Train: 1.4748, Val: 1.4993\n",
            "Epoch: 013, Loss: 2.1749, Train: 1.2477, Val: 1.2691\n",
            "Epoch: 014, Loss: 1.5567, Train: 1.0510, Val: 1.0619\n",
            "Epoch: 015, Loss: 1.1046, Train: 1.0576, Val: 1.0505\n",
            "Epoch: 016, Loss: 1.1185, Train: 1.2259, Val: 1.2053\n",
            "Epoch: 017, Loss: 1.5029, Train: 1.3110, Val: 1.2867\n",
            "Epoch: 018, Loss: 1.7186, Train: 1.2174, Val: 1.1976\n",
            "Epoch: 019, Loss: 1.4820, Train: 1.0658, Val: 1.0575\n",
            "Epoch: 020, Loss: 1.1359, Train: 1.0124, Val: 1.0182\n",
            "Epoch: 021, Loss: 1.0250, Train: 1.0684, Val: 1.0835\n",
            "Epoch: 022, Loss: 1.1414, Train: 1.1415, Val: 1.1606\n",
            "Epoch: 023, Loss: 1.3030, Train: 1.1695, Val: 1.1898\n",
            "Epoch: 024, Loss: 1.3677, Train: 1.1394, Val: 1.1589\n",
            "Epoch: 025, Loss: 1.2983, Train: 1.0711, Val: 1.0876\n",
            "Epoch: 026, Loss: 1.1472, Train: 1.0092, Val: 1.0197\n",
            "Epoch: 027, Loss: 1.0185, Train: 1.0017, Val: 1.0035\n",
            "Epoch: 028, Loss: 1.0034, Train: 1.0463, Val: 1.0405\n",
            "Epoch: 029, Loss: 1.0948, Train: 1.0828, Val: 1.0736\n",
            "Epoch: 030, Loss: 1.1725, Train: 1.0666, Val: 1.0589\n",
            "Epoch: 031, Loss: 1.1377, Train: 1.0171, Val: 1.0149\n",
            "Epoch: 032, Loss: 1.0345, Train: 0.9853, Val: 0.9906\n",
            "Epoch: 033, Loss: 0.9708, Train: 0.9927, Val: 1.0044\n",
            "Epoch: 034, Loss: 0.9855, Train: 1.0176, Val: 1.0332\n",
            "Epoch: 035, Loss: 1.0356, Train: 1.0303, Val: 1.0472\n",
            "Epoch: 036, Loss: 1.0616, Train: 1.0190, Val: 1.0354\n",
            "Epoch: 037, Loss: 1.0383, Train: 0.9925, Val: 1.0065\n",
            "Epoch: 038, Loss: 0.9850, Train: 0.9720, Val: 0.9819\n",
            "Epoch: 039, Loss: 0.9448, Train: 0.9730, Val: 0.9781\n",
            "Epoch: 040, Loss: 0.9468, Train: 0.9880, Val: 0.9894\n",
            "Epoch: 041, Loss: 0.9762, Train: 0.9944, Val: 0.9946\n",
            "Epoch: 042, Loss: 0.9889, Train: 0.9823, Val: 0.9843\n",
            "Epoch: 043, Loss: 0.9649, Train: 0.9643, Val: 0.9701\n",
            "Epoch: 044, Loss: 0.9298, Train: 0.9572, Val: 0.9673\n",
            "Epoch: 045, Loss: 0.9162, Train: 0.9626, Val: 0.9760\n",
            "Epoch: 046, Loss: 0.9266, Train: 0.9690, Val: 0.9842\n",
            "Epoch: 047, Loss: 0.9390, Train: 0.9669, Val: 0.9822\n",
            "Epoch: 048, Loss: 0.9349, Train: 0.9567, Val: 0.9707\n",
            "Epoch: 049, Loss: 0.9152, Train: 0.9469, Val: 0.9585\n",
            "Epoch: 050, Loss: 0.8967, Train: 0.9451, Val: 0.9539\n",
            "Epoch: 051, Loss: 0.8933, Train: 0.9492, Val: 0.9557\n",
            "Epoch: 052, Loss: 0.9010, Train: 0.9504, Val: 0.9562\n",
            "Epoch: 053, Loss: 0.9033, Train: 0.9448, Val: 0.9517\n",
            "Epoch: 054, Loss: 0.8926, Train: 0.9372, Val: 0.9465\n",
            "Epoch: 055, Loss: 0.8784, Train: 0.9343, Val: 0.9461\n",
            "Epoch: 056, Loss: 0.8728, Train: 0.9358, Val: 0.9495\n",
            "Epoch: 057, Loss: 0.8756, Train: 0.9366, Val: 0.9513\n",
            "Epoch: 058, Loss: 0.8773, Train: 0.9337, Val: 0.9481\n",
            "Epoch: 059, Loss: 0.8717, Train: 0.9285, Val: 0.9418\n",
            "Epoch: 060, Loss: 0.8622, Train: 0.9255, Val: 0.9369\n",
            "Epoch: 061, Loss: 0.8565, Train: 0.9257, Val: 0.9355\n",
            "Epoch: 062, Loss: 0.8569, Train: 0.9264, Val: 0.9352\n",
            "Epoch: 063, Loss: 0.8581, Train: 0.9244, Val: 0.9336\n",
            "Epoch: 064, Loss: 0.8546, Train: 0.9209, Val: 0.9314\n",
            "Epoch: 065, Loss: 0.8481, Train: 0.9189, Val: 0.9310\n",
            "Epoch: 066, Loss: 0.8443, Train: 0.9189, Val: 0.9324\n",
            "Epoch: 067, Loss: 0.8445, Train: 0.9191, Val: 0.9333\n",
            "Epoch: 068, Loss: 0.8448, Train: 0.9177, Val: 0.9317\n",
            "Epoch: 069, Loss: 0.8422, Train: 0.9155, Val: 0.9286\n",
            "Epoch: 070, Loss: 0.8382, Train: 0.9143, Val: 0.9262\n",
            "Epoch: 071, Loss: 0.8359, Train: 0.9144, Val: 0.9253\n",
            "Epoch: 072, Loss: 0.8362, Train: 0.9144, Val: 0.9249\n",
            "Epoch: 073, Loss: 0.8362, Train: 0.9133, Val: 0.9241\n",
            "Epoch: 074, Loss: 0.8341, Train: 0.9119, Val: 0.9237\n",
            "Epoch: 075, Loss: 0.8316, Train: 0.9114, Val: 0.9243\n",
            "Epoch: 076, Loss: 0.8307, Train: 0.9116, Val: 0.9252\n",
            "Epoch: 077, Loss: 0.8309, Train: 0.9113, Val: 0.9252\n",
            "Epoch: 078, Loss: 0.8305, Train: 0.9104, Val: 0.9239\n",
            "Epoch: 079, Loss: 0.8289, Train: 0.9097, Val: 0.9224\n",
            "Epoch: 080, Loss: 0.8275, Train: 0.9095, Val: 0.9215\n",
            "Epoch: 081, Loss: 0.8272, Train: 0.9095, Val: 0.9212\n",
            "Epoch: 082, Loss: 0.8272, Train: 0.9091, Val: 0.9209\n",
            "Epoch: 083, Loss: 0.8265, Train: 0.9084, Val: 0.9209\n",
            "Epoch: 084, Loss: 0.8252, Train: 0.9080, Val: 0.9213\n",
            "Epoch: 085, Loss: 0.8245, Train: 0.9079, Val: 0.9219\n",
            "Epoch: 086, Loss: 0.8244, Train: 0.9077, Val: 0.9219\n",
            "Epoch: 087, Loss: 0.8240, Train: 0.9072, Val: 0.9213\n",
            "Epoch: 088, Loss: 0.8231, Train: 0.9068, Val: 0.9203\n",
            "Epoch: 089, Loss: 0.8222, Train: 0.9065, Val: 0.9197\n",
            "Epoch: 090, Loss: 0.8218, Train: 0.9064, Val: 0.9194\n",
            "Epoch: 091, Loss: 0.8215, Train: 0.9060, Val: 0.9193\n",
            "Epoch: 092, Loss: 0.8209, Train: 0.9056, Val: 0.9193\n",
            "Epoch: 093, Loss: 0.8200, Train: 0.9052, Val: 0.9196\n",
            "Epoch: 094, Loss: 0.8195, Train: 0.9050, Val: 0.9199\n",
            "Epoch: 095, Loss: 0.8191, Train: 0.9048, Val: 0.9198\n",
            "Epoch: 096, Loss: 0.8186, Train: 0.9044, Val: 0.9194\n",
            "Epoch: 097, Loss: 0.8179, Train: 0.9040, Val: 0.9188\n",
            "Epoch: 098, Loss: 0.8173, Train: 0.9038, Val: 0.9184\n",
            "Epoch: 099, Loss: 0.8168, Train: 0.9035, Val: 0.9182\n",
            "Epoch: 100, Loss: 0.8164, Train: 0.9032, Val: 0.9181\n",
            "Epoch: 101, Loss: 0.8158, Train: 0.9028, Val: 0.9182\n",
            "Epoch: 102, Loss: 0.8151, Train: 0.9026, Val: 0.9184\n",
            "Epoch: 103, Loss: 0.8146, Train: 0.9023, Val: 0.9185\n",
            "Epoch: 104, Loss: 0.8142, Train: 0.9020, Val: 0.9183\n",
            "Epoch: 105, Loss: 0.8136, Train: 0.9017, Val: 0.9179\n",
            "Epoch: 106, Loss: 0.8130, Train: 0.9014, Val: 0.9176\n",
            "Epoch: 107, Loss: 0.8125, Train: 0.9011, Val: 0.9173\n",
            "Epoch: 108, Loss: 0.8120, Train: 0.9008, Val: 0.9172\n",
            "Epoch: 109, Loss: 0.8115, Train: 0.9005, Val: 0.9172\n",
            "Epoch: 110, Loss: 0.8109, Train: 0.9002, Val: 0.9173\n",
            "Epoch: 111, Loss: 0.8104, Train: 0.8999, Val: 0.9173\n",
            "Epoch: 112, Loss: 0.8099, Train: 0.8996, Val: 0.9172\n",
            "Epoch: 113, Loss: 0.8093, Train: 0.8993, Val: 0.9169\n",
            "Epoch: 114, Loss: 0.8087, Train: 0.8990, Val: 0.9166\n",
            "Epoch: 115, Loss: 0.8082, Train: 0.8987, Val: 0.9164\n",
            "Epoch: 116, Loss: 0.8076, Train: 0.8984, Val: 0.9162\n",
            "Epoch: 117, Loss: 0.8071, Train: 0.8980, Val: 0.9161\n",
            "Epoch: 118, Loss: 0.8065, Train: 0.8977, Val: 0.9161\n",
            "Epoch: 119, Loss: 0.8059, Train: 0.8974, Val: 0.9161\n",
            "Epoch: 120, Loss: 0.8053, Train: 0.8971, Val: 0.9160\n",
            "Epoch: 121, Loss: 0.8047, Train: 0.8967, Val: 0.9157\n",
            "Epoch: 122, Loss: 0.8041, Train: 0.8964, Val: 0.9154\n",
            "Epoch: 123, Loss: 0.8035, Train: 0.8961, Val: 0.9152\n",
            "Epoch: 124, Loss: 0.8029, Train: 0.8957, Val: 0.9150\n",
            "Epoch: 125, Loss: 0.8023, Train: 0.8954, Val: 0.9149\n",
            "Epoch: 126, Loss: 0.8017, Train: 0.8950, Val: 0.9148\n",
            "Epoch: 127, Loss: 0.8011, Train: 0.8947, Val: 0.9147\n",
            "Epoch: 128, Loss: 0.8005, Train: 0.8943, Val: 0.9145\n",
            "Epoch: 129, Loss: 0.7998, Train: 0.8940, Val: 0.9143\n",
            "Epoch: 130, Loss: 0.7992, Train: 0.8936, Val: 0.9140\n",
            "Epoch: 131, Loss: 0.7986, Train: 0.8932, Val: 0.9138\n",
            "Epoch: 132, Loss: 0.7979, Train: 0.8929, Val: 0.9136\n",
            "Epoch: 133, Loss: 0.7973, Train: 0.8925, Val: 0.9134\n",
            "Epoch: 134, Loss: 0.7966, Train: 0.8921, Val: 0.9133\n",
            "Epoch: 135, Loss: 0.7959, Train: 0.8918, Val: 0.9132\n",
            "Epoch: 136, Loss: 0.7953, Train: 0.8914, Val: 0.9130\n",
            "Epoch: 137, Loss: 0.7946, Train: 0.8910, Val: 0.9127\n",
            "Epoch: 138, Loss: 0.7939, Train: 0.8906, Val: 0.9125\n",
            "Epoch: 139, Loss: 0.7932, Train: 0.8902, Val: 0.9123\n",
            "Epoch: 140, Loss: 0.7925, Train: 0.8898, Val: 0.9121\n",
            "Epoch: 141, Loss: 0.7918, Train: 0.8894, Val: 0.9119\n",
            "Epoch: 142, Loss: 0.7911, Train: 0.8890, Val: 0.9117\n",
            "Epoch: 143, Loss: 0.7904, Train: 0.8886, Val: 0.9116\n",
            "Epoch: 144, Loss: 0.7897, Train: 0.8882, Val: 0.9113\n",
            "Epoch: 145, Loss: 0.7890, Train: 0.8878, Val: 0.9111\n",
            "Epoch: 146, Loss: 0.7883, Train: 0.8874, Val: 0.9109\n",
            "Epoch: 147, Loss: 0.7876, Train: 0.8870, Val: 0.9106\n",
            "Epoch: 148, Loss: 0.7868, Train: 0.8866, Val: 0.9104\n",
            "Epoch: 149, Loss: 0.7861, Train: 0.8862, Val: 0.9102\n",
            "Epoch: 150, Loss: 0.7853, Train: 0.8857, Val: 0.9101\n",
            "Epoch: 151, Loss: 0.7846, Train: 0.8853, Val: 0.9098\n",
            "Epoch: 152, Loss: 0.7838, Train: 0.8849, Val: 0.9096\n",
            "Epoch: 153, Loss: 0.7831, Train: 0.8844, Val: 0.9093\n",
            "Epoch: 154, Loss: 0.7823, Train: 0.8840, Val: 0.9091\n",
            "Epoch: 155, Loss: 0.7816, Train: 0.8836, Val: 0.9089\n",
            "Epoch: 156, Loss: 0.7808, Train: 0.8831, Val: 0.9087\n",
            "Epoch: 157, Loss: 0.7800, Train: 0.8827, Val: 0.9085\n",
            "Epoch: 158, Loss: 0.7792, Train: 0.8822, Val: 0.9083\n",
            "Epoch: 159, Loss: 0.7785, Train: 0.8818, Val: 0.9080\n",
            "Epoch: 160, Loss: 0.7777, Train: 0.8814, Val: 0.9078\n",
            "Epoch: 161, Loss: 0.7769, Train: 0.8809, Val: 0.9075\n",
            "Epoch: 162, Loss: 0.7761, Train: 0.8805, Val: 0.9073\n",
            "Epoch: 163, Loss: 0.7753, Train: 0.8800, Val: 0.9071\n",
            "Epoch: 164, Loss: 0.7745, Train: 0.8796, Val: 0.9068\n",
            "Epoch: 165, Loss: 0.7737, Train: 0.8791, Val: 0.9066\n",
            "Epoch: 166, Loss: 0.7729, Train: 0.8786, Val: 0.9064\n",
            "Epoch: 167, Loss: 0.7721, Train: 0.8782, Val: 0.9061\n",
            "Epoch: 168, Loss: 0.7713, Train: 0.8777, Val: 0.9059\n",
            "Epoch: 169, Loss: 0.7706, Train: 0.8773, Val: 0.9056\n",
            "Epoch: 170, Loss: 0.7698, Train: 0.8768, Val: 0.9054\n",
            "Epoch: 171, Loss: 0.7690, Train: 0.8764, Val: 0.9052\n",
            "Epoch: 172, Loss: 0.7682, Train: 0.8759, Val: 0.9050\n",
            "Epoch: 173, Loss: 0.7674, Train: 0.8755, Val: 0.9048\n",
            "Epoch: 174, Loss: 0.7666, Train: 0.8750, Val: 0.9045\n",
            "Epoch: 175, Loss: 0.7658, Train: 0.8746, Val: 0.9043\n",
            "Epoch: 176, Loss: 0.7650, Train: 0.8741, Val: 0.9041\n",
            "Epoch: 177, Loss: 0.7642, Train: 0.8737, Val: 0.9039\n",
            "Epoch: 178, Loss: 0.7635, Train: 0.8732, Val: 0.9037\n",
            "Epoch: 179, Loss: 0.7627, Train: 0.8728, Val: 0.9035\n",
            "Epoch: 180, Loss: 0.7619, Train: 0.8723, Val: 0.9032\n",
            "Epoch: 181, Loss: 0.7612, Train: 0.8719, Val: 0.9030\n",
            "Epoch: 182, Loss: 0.7604, Train: 0.8715, Val: 0.9028\n",
            "Epoch: 183, Loss: 0.7597, Train: 0.8710, Val: 0.9026\n",
            "Epoch: 184, Loss: 0.7589, Train: 0.8706, Val: 0.9025\n",
            "Epoch: 185, Loss: 0.7582, Train: 0.8702, Val: 0.9023\n",
            "Epoch: 186, Loss: 0.7575, Train: 0.8698, Val: 0.9021\n",
            "Epoch: 187, Loss: 0.7567, Train: 0.8694, Val: 0.9019\n",
            "Epoch: 188, Loss: 0.7560, Train: 0.8690, Val: 0.9018\n",
            "Epoch: 189, Loss: 0.7553, Train: 0.8686, Val: 0.9016\n",
            "Epoch: 190, Loss: 0.7546, Train: 0.8682, Val: 0.9014\n",
            "Epoch: 191, Loss: 0.7540, Train: 0.8678, Val: 0.9013\n",
            "Epoch: 192, Loss: 0.7533, Train: 0.8674, Val: 0.9012\n",
            "Epoch: 193, Loss: 0.7526, Train: 0.8670, Val: 0.9010\n",
            "Epoch: 194, Loss: 0.7520, Train: 0.8667, Val: 0.9009\n",
            "Epoch: 195, Loss: 0.7514, Train: 0.8663, Val: 0.9008\n",
            "Epoch: 196, Loss: 0.7507, Train: 0.8659, Val: 0.9007\n",
            "Epoch: 197, Loss: 0.7501, Train: 0.8656, Val: 0.9006\n",
            "Epoch: 198, Loss: 0.7495, Train: 0.8653, Val: 0.9005\n",
            "Epoch: 199, Loss: 0.7490, Train: 0.8649, Val: 0.9004\n",
            "Epoch: 200, Loss: 0.7484, Train: 0.8646, Val: 0.9003\n",
            "Epoch: 201, Loss: 0.7479, Train: 0.8643, Val: 0.9002\n",
            "Epoch: 202, Loss: 0.7473, Train: 0.8640, Val: 0.9001\n",
            "Epoch: 203, Loss: 0.7468, Train: 0.8637, Val: 0.9001\n",
            "Epoch: 204, Loss: 0.7463, Train: 0.8634, Val: 0.9000\n",
            "Epoch: 205, Loss: 0.7458, Train: 0.8631, Val: 0.9000\n",
            "Epoch: 206, Loss: 0.7453, Train: 0.8628, Val: 0.8999\n",
            "Epoch: 207, Loss: 0.7448, Train: 0.8626, Val: 0.8999\n",
            "Epoch: 208, Loss: 0.7444, Train: 0.8623, Val: 0.8998\n",
            "Epoch: 209, Loss: 0.7439, Train: 0.8621, Val: 0.8998\n",
            "Epoch: 210, Loss: 0.7435, Train: 0.8618, Val: 0.8998\n",
            "Epoch: 211, Loss: 0.7431, Train: 0.8616, Val: 0.8997\n",
            "Epoch: 212, Loss: 0.7427, Train: 0.8613, Val: 0.8997\n",
            "Epoch: 213, Loss: 0.7423, Train: 0.8611, Val: 0.8997\n",
            "Epoch: 214, Loss: 0.7419, Train: 0.8609, Val: 0.8997\n",
            "Epoch: 215, Loss: 0.7416, Train: 0.8607, Val: 0.8996\n",
            "Epoch: 216, Loss: 0.7412, Train: 0.8605, Val: 0.8996\n",
            "Epoch: 217, Loss: 0.7409, Train: 0.8603, Val: 0.8996\n",
            "Epoch: 218, Loss: 0.7405, Train: 0.8601, Val: 0.8996\n",
            "Epoch: 219, Loss: 0.7402, Train: 0.8599, Val: 0.8995\n",
            "Epoch: 220, Loss: 0.7399, Train: 0.8597, Val: 0.8995\n",
            "Epoch: 221, Loss: 0.7396, Train: 0.8596, Val: 0.8995\n",
            "Epoch: 222, Loss: 0.7393, Train: 0.8594, Val: 0.8995\n",
            "Epoch: 223, Loss: 0.7390, Train: 0.8592, Val: 0.8994\n",
            "Epoch: 224, Loss: 0.7387, Train: 0.8591, Val: 0.8994\n",
            "Epoch: 225, Loss: 0.7385, Train: 0.8589, Val: 0.8994\n",
            "Epoch: 226, Loss: 0.7382, Train: 0.8588, Val: 0.8994\n",
            "Epoch: 227, Loss: 0.7380, Train: 0.8586, Val: 0.8993\n",
            "Epoch: 228, Loss: 0.7377, Train: 0.8585, Val: 0.8993\n",
            "Epoch: 229, Loss: 0.7375, Train: 0.8583, Val: 0.8993\n",
            "Epoch: 230, Loss: 0.7372, Train: 0.8582, Val: 0.8993\n",
            "Epoch: 231, Loss: 0.7370, Train: 0.8581, Val: 0.8992\n",
            "Epoch: 232, Loss: 0.7368, Train: 0.8579, Val: 0.8992\n",
            "Epoch: 233, Loss: 0.7365, Train: 0.8578, Val: 0.8991\n",
            "Epoch: 234, Loss: 0.7363, Train: 0.8577, Val: 0.8991\n",
            "Epoch: 235, Loss: 0.7361, Train: 0.8576, Val: 0.8991\n",
            "Epoch: 236, Loss: 0.7359, Train: 0.8574, Val: 0.8990\n",
            "Epoch: 237, Loss: 0.7357, Train: 0.8573, Val: 0.8990\n",
            "Epoch: 238, Loss: 0.7355, Train: 0.8572, Val: 0.8989\n",
            "Epoch: 239, Loss: 0.7353, Train: 0.8571, Val: 0.8989\n",
            "Epoch: 240, Loss: 0.7351, Train: 0.8570, Val: 0.8988\n",
            "Epoch: 241, Loss: 0.7349, Train: 0.8569, Val: 0.8988\n",
            "Epoch: 242, Loss: 0.7347, Train: 0.8568, Val: 0.8987\n",
            "Epoch: 243, Loss: 0.7345, Train: 0.8566, Val: 0.8986\n",
            "Epoch: 244, Loss: 0.7344, Train: 0.8565, Val: 0.8986\n",
            "Epoch: 245, Loss: 0.7342, Train: 0.8564, Val: 0.8985\n",
            "Epoch: 246, Loss: 0.7340, Train: 0.8563, Val: 0.8984\n",
            "Epoch: 247, Loss: 0.7338, Train: 0.8562, Val: 0.8984\n",
            "Epoch: 248, Loss: 0.7337, Train: 0.8561, Val: 0.8983\n",
            "Epoch: 249, Loss: 0.7335, Train: 0.8560, Val: 0.8982\n",
            "Epoch: 250, Loss: 0.7333, Train: 0.8559, Val: 0.8982\n",
            "Epoch: 251, Loss: 0.7332, Train: 0.8558, Val: 0.8981\n",
            "Epoch: 252, Loss: 0.7330, Train: 0.8558, Val: 0.8980\n",
            "Epoch: 253, Loss: 0.7328, Train: 0.8557, Val: 0.8980\n",
            "Epoch: 254, Loss: 0.7327, Train: 0.8556, Val: 0.8979\n",
            "Epoch: 255, Loss: 0.7325, Train: 0.8555, Val: 0.8978\n",
            "Epoch: 256, Loss: 0.7324, Train: 0.8554, Val: 0.8978\n",
            "Epoch: 257, Loss: 0.7322, Train: 0.8553, Val: 0.8977\n",
            "Epoch: 258, Loss: 0.7321, Train: 0.8552, Val: 0.8976\n",
            "Epoch: 259, Loss: 0.7319, Train: 0.8551, Val: 0.8976\n",
            "Epoch: 260, Loss: 0.7318, Train: 0.8550, Val: 0.8975\n",
            "Epoch: 261, Loss: 0.7316, Train: 0.8550, Val: 0.8974\n",
            "Epoch: 262, Loss: 0.7315, Train: 0.8549, Val: 0.8974\n",
            "Epoch: 263, Loss: 0.7313, Train: 0.8548, Val: 0.8973\n",
            "Epoch: 264, Loss: 0.7312, Train: 0.8547, Val: 0.8973\n",
            "Epoch: 265, Loss: 0.7311, Train: 0.8546, Val: 0.8972\n",
            "Epoch: 266, Loss: 0.7309, Train: 0.8545, Val: 0.8971\n",
            "Epoch: 267, Loss: 0.7308, Train: 0.8545, Val: 0.8971\n",
            "Epoch: 268, Loss: 0.7306, Train: 0.8544, Val: 0.8970\n",
            "Epoch: 269, Loss: 0.7305, Train: 0.8543, Val: 0.8970\n",
            "Epoch: 270, Loss: 0.7304, Train: 0.8542, Val: 0.8969\n",
            "Epoch: 271, Loss: 0.7302, Train: 0.8542, Val: 0.8968\n",
            "Epoch: 272, Loss: 0.7301, Train: 0.8541, Val: 0.8968\n",
            "Epoch: 273, Loss: 0.7300, Train: 0.8540, Val: 0.8967\n",
            "Epoch: 274, Loss: 0.7299, Train: 0.8539, Val: 0.8967\n",
            "Epoch: 275, Loss: 0.7297, Train: 0.8539, Val: 0.8966\n",
            "Epoch: 276, Loss: 0.7296, Train: 0.8538, Val: 0.8966\n",
            "Epoch: 277, Loss: 0.7295, Train: 0.8537, Val: 0.8965\n",
            "Epoch: 278, Loss: 0.7294, Train: 0.8536, Val: 0.8965\n",
            "Epoch: 279, Loss: 0.7292, Train: 0.8536, Val: 0.8964\n",
            "Epoch: 280, Loss: 0.7291, Train: 0.8535, Val: 0.8964\n",
            "Epoch: 281, Loss: 0.7290, Train: 0.8534, Val: 0.8963\n",
            "Epoch: 282, Loss: 0.7289, Train: 0.8533, Val: 0.8963\n",
            "Epoch: 283, Loss: 0.7287, Train: 0.8533, Val: 0.8962\n",
            "Epoch: 284, Loss: 0.7286, Train: 0.8532, Val: 0.8962\n",
            "Epoch: 285, Loss: 0.7285, Train: 0.8531, Val: 0.8961\n",
            "Epoch: 286, Loss: 0.7284, Train: 0.8531, Val: 0.8961\n",
            "Epoch: 287, Loss: 0.7283, Train: 0.8530, Val: 0.8961\n",
            "Epoch: 288, Loss: 0.7281, Train: 0.8529, Val: 0.8960\n",
            "Epoch: 289, Loss: 0.7280, Train: 0.8529, Val: 0.8960\n",
            "Epoch: 290, Loss: 0.7279, Train: 0.8528, Val: 0.8959\n",
            "Epoch: 291, Loss: 0.7278, Train: 0.8527, Val: 0.8959\n",
            "Epoch: 292, Loss: 0.7277, Train: 0.8527, Val: 0.8959\n",
            "Epoch: 293, Loss: 0.7276, Train: 0.8526, Val: 0.8958\n",
            "Epoch: 294, Loss: 0.7275, Train: 0.8525, Val: 0.8958\n",
            "Epoch: 295, Loss: 0.7273, Train: 0.8525, Val: 0.8958\n",
            "Epoch: 296, Loss: 0.7272, Train: 0.8524, Val: 0.8957\n",
            "Epoch: 297, Loss: 0.7271, Train: 0.8523, Val: 0.8957\n",
            "Epoch: 298, Loss: 0.7270, Train: 0.8523, Val: 0.8957\n",
            "Epoch: 299, Loss: 0.7269, Train: 0.8522, Val: 0.8956\n",
            "Epoch: 300, Loss: 0.7268, Train: 0.8521, Val: 0.8956\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    pred = model(train_data.x_dict, train_data.edge_index_dict,\n",
        "                 train_data['user', 'movie'].edge_label_index)\n",
        "    target = train_data['user', 'movie'].edge_label\n",
        "    loss = F.mse_loss(pred, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return float(loss)\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(data):\n",
        "    data = data.to(device)\n",
        "    model.eval()\n",
        "    pred = model(data.x_dict, data.edge_index_dict,\n",
        "                 data['user', 'movie'].edge_label_index)\n",
        "    pred = pred.clamp(min=0, max=5)\n",
        "    target = data['user', 'movie'].edge_label.float()\n",
        "    rmse = F.mse_loss(pred, target).sqrt()\n",
        "    return float(rmse)\n",
        "\n",
        "\n",
        "for epoch in range(1, 301):\n",
        "    train_data = train_data.to(device)\n",
        "    loss = train()\n",
        "    train_rmse = test(train_data)\n",
        "    val_rmse = test(val_data)\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_rmse:.4f}, '\n",
        "          f'Val: {val_rmse:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGP6RiiZJtU9"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "From the validation results, our model can generalize well to unseen data. The val RMSE is should be around 0.9, meaning that, on average our model is off by 0.9 stars. We can now evaluate our model on the test set and take a closer look into the predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QT_NONRIJwE6",
        "outputId": "e10c6e0f-b03b-4129-c11d-e2c0ca4d0288"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test RMSE: 0.8866\n",
            "       userId  movieId    rating  target\n",
            "0         248     4001  4.390224     4.0\n",
            "1         551       31  3.221296     1.5\n",
            "2         483      323  3.955466     5.0\n",
            "3         488     1461  2.677859     5.0\n",
            "4          90     1145  3.330203     3.0\n",
            "...       ...      ...       ...     ...\n",
            "10079     476     7087  4.674701     4.5\n",
            "10080     413     1640  3.235874     2.0\n",
            "10081     424      739  3.419750     3.5\n",
            "10082     225     1875  3.267421     4.0\n",
            "10083     524     1440  3.646559     3.5\n",
            "\n",
            "[10084 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    test_data = test_data.to(device)\n",
        "    pred = model(test_data.x_dict, test_data.edge_index_dict,\n",
        "                 test_data['user', 'movie'].edge_label_index)\n",
        "    pred = pred.clamp(min=0, max=5)\n",
        "    target = test_data['user', 'movie'].edge_label.float()\n",
        "    rmse = F.mse_loss(pred, target).sqrt()\n",
        "    print(f'Test RMSE: {rmse:.4f}')\n",
        "\n",
        "userId = test_data['user', 'movie'].edge_label_index[0].cpu().numpy()\n",
        "movieId = test_data['user', 'movie'].edge_label_index[1].cpu().numpy()\n",
        "pred = pred.cpu().numpy()\n",
        "target = target.cpu().numpy()\n",
        "\n",
        "print(pd.DataFrame({'userId': userId, 'movieId': movieId, 'rating': pred, 'target': target}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWH_UzPYF7_I"
      },
      "source": [
        "## Movie recommendations\n",
        "\n",
        "We can now use the model to generate ratings for a movie we haven't seen.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gp3hABvCoFP",
        "outputId": "71319d64-bf22-4995-c4fe-a5cfbab388f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The movie we want to predict a raiting for is:  Man in the White Suit, The (1951)\n"
          ]
        }
      ],
      "source": [
        "# Your mappedUserId\n",
        "mapped_user_id = unique_user_id[unique_user_id['userId'] == our_user_id]['mappedUserId'].values[0]\n",
        "\n",
        "# Select movies that you haven't seen before\n",
        "movies_rated = ratings_df[ratings_df['mappedUserId'] == mapped_user_id]\n",
        "movies_not_rated = movies_df[~movies_df.index.isin(movies_rated['movieId'])]\n",
        "movies_not_rated = movies_not_rated.merge(unique_movie_id, on='movieId')\n",
        "movie = movies_not_rated.sample(1)\n",
        "\n",
        "print(f\"The movie we want to predict a raiting for is:  {movie['title'].item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYe9poFGEig6"
      },
      "outputs": [],
      "source": [
        "# Create new `edge_label_index` between the user and the movie\n",
        "edge_label_index = torch.tensor([\n",
        "    mapped_user_id,\n",
        "    movie.mappedMovieId.item()])\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_data.to(device)\n",
        "    pred = model(test_data.x_dict, test_data.edge_index_dict, edge_label_index)\n",
        "    pred = pred.clamp(min=0, max=5).detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GN5uxt-CD1lV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db5495a2-15ac-4744-d04f-65fbcba6062b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.706182479858398"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "pred.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyhTL520KFwC"
      },
      "source": [
        "## Explaining the Predictions\n",
        "\n",
        "PyTorch Geometric also provides a way to explain the predictions of a GNN. Let's check which movie ratings have influenced this prediction the most.\n",
        "\n",
        "We will use the [captum](https://captum.ai/) library to explain the predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1fe7gcrPNo0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "704af144-af80-42f0-abe6-a41ac1a2e717"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HeteroExplanation(\n",
              "  prediction=[1],\n",
              "  target=[1],\n",
              "  index=[1],\n",
              "  edge_label_index=[2],\n",
              "  user={ x=[611, 611] },\n",
              "  movie={ x=[9742, 404] },\n",
              "  (user, rates, movie)={\n",
              "    edge_mask=[90757],\n",
              "    edge_index=[2, 90757],\n",
              "  },\n",
              "  (movie, rev_rates, user)={\n",
              "    edge_mask=[90757],\n",
              "    edge_index=[2, 90757],\n",
              "  }\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "from torch_geometric.explain import Explainer, CaptumExplainer\n",
        "\n",
        "explainer = Explainer(\n",
        "    model=model,\n",
        "    algorithm=CaptumExplainer('IntegratedGradients'),\n",
        "    explanation_type='model',\n",
        "    model_config=dict(\n",
        "        mode='regression',\n",
        "        task_level='edge',\n",
        "        return_type='raw',\n",
        "    ),\n",
        "    node_mask_type=None,\n",
        "    edge_mask_type='object',\n",
        ")\n",
        "\n",
        "explanation = explainer(\n",
        "    test_data.x_dict, test_data.edge_index_dict, index=0,\n",
        "    edge_label_index=edge_label_index).cpu().detach()\n",
        "explanation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YIoR6LOGLa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3abf767c-0731-4a99-e9ea-899092a6f423"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attribtion for all edges towards prediction of movie rating of movie:\n",
            " Man in the White Suit, The (1951)\n",
            "==========================================================================================\n",
            "       mappedUserId  mappedMovieId      attr\n",
            "32711           610           8652 -0.000828\n",
            "29664           351           3629 -0.000259\n",
            "70968           610           3629 -0.000225\n",
            "12973           513           8652 -0.000137\n",
            "10054           598           3629 -0.000086\n",
            "...             ...            ...       ...\n",
            "32711           610           8652  0.042021\n",
            "24303           610           1129  0.070826\n",
            "34228           610            242  0.084293\n",
            "70968           610           3629  0.086883\n",
            "64200           473           8261  0.181048\n",
            "\n",
            "[181514 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "# User to movie link + attribution\n",
        "user_to_movie = explanation['user', 'movie'].edge_index.numpy().T\n",
        "user_to_movie_attr = explanation['user', 'movie'].edge_mask.numpy().T\n",
        "user_to_movie_df = pd.DataFrame(\n",
        "    np.hstack([user_to_movie, user_to_movie_attr.reshape(-1,1)]),\n",
        "    columns = ['mappedUserId', 'mappedMovieId', 'attr']\n",
        ")\n",
        "\n",
        "# Movie to user link + attribution\n",
        "movie_to_user = explanation['movie', 'user'].edge_index.numpy().T\n",
        "movie_to_user_attr = explanation[ 'movie', 'user'].edge_mask.numpy().T\n",
        "movie_to_user_df = pd.DataFrame(\n",
        "    np.hstack([movie_to_user, movie_to_user_attr.reshape(-1,1)]),\n",
        "    columns = ['mappedMovieId', 'mappedUserId','attr']\n",
        ")\n",
        "explanation_df = pd.concat([user_to_movie_df, movie_to_user_df])\n",
        "explanation_df[[\"mappedUserId\", \"mappedMovieId\"]] = explanation_df[[\"mappedUserId\", \"mappedMovieId\"]].astype(int)\n",
        "\n",
        "print(f\"Attribtion for all edges towards prediction of movie rating of movie:\\n {movie['title'].item()}\")\n",
        "print(\"==========================================================================================\")\n",
        "print(explanation_df.sort_values(by='attr'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yN6svXQKe7C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02417933-d92a-4c8f-9b8c-85232b44899f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top movies that influenced the prediction:\n",
            "==============================================\n",
            "                                          title        attr\n",
            "2                                In Time (2011) 0.086657975\n",
            "0                         Shutter Island (2010) 0.084278290\n",
            "1  Eternal Sunshine of the Spotless Mind (2004) 0.070820077\n",
            "3                            The Shining (1997) 0.041193223\n"
          ]
        }
      ],
      "source": [
        "# Select links that connect to our user\n",
        "explanation_df = explanation_df[explanation_df['mappedUserId'] == mapped_user_id]\n",
        "\n",
        "# We group the attribution scores by movie\n",
        "explanation_df = explanation_df.groupby('mappedMovieId').sum()\n",
        "\n",
        "# Merge with movies_df to receive title\n",
        "# But first, we need to add the original id\n",
        "explanation_df = explanation_df.merge(unique_movie_id, on='mappedMovieId')\n",
        "explanation_df = explanation_df.merge(movies_df, on='movieId')\n",
        "\n",
        "pd.options.display.float_format = \"{:,.9f}\".format\n",
        "\n",
        "print(\"Top movies that influenced the prediction:\")\n",
        "print(\"==============================================\")\n",
        "print(explanation_df.sort_values(by='attr', ascending=False, key= lambda x: abs(x))[['title', 'attr']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find more on the official PyTorch Geometric website [here](https://www.pyg.org/)."
      ],
      "metadata": {
        "id": "mvUa8MyDFj-V"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f832a6504b7d4797b2f76aea083c7d30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc388e3418e747a9861c5d5d93514d75",
              "IPY_MODEL_a9e6e732503c462ea4e22a88235a0542",
              "IPY_MODEL_548f0ec5c73b41f09b524eeeae123990"
            ],
            "layout": "IPY_MODEL_70ffd13684be4cea9306fadaebb743f5"
          }
        },
        "cc388e3418e747a9861c5d5d93514d75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73fec9bd6c4645a5a78bd4d27cc171f3",
            "placeholder": "​",
            "style": "IPY_MODEL_05f7c31612a341018fb0812105d2b37c",
            "value": "Batches: 100%"
          }
        },
        "a9e6e732503c462ea4e22a88235a0542": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_831622595932480fba9d6e7a58dce41b",
            "max": 305,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_670f5c889f9748c7b2a37cfecc674022",
            "value": 305
          }
        },
        "548f0ec5c73b41f09b524eeeae123990": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_716875445e814266aa06d408d6e7edc2",
            "placeholder": "​",
            "style": "IPY_MODEL_7ac4ce33a1f74874997ec89cd0723b16",
            "value": " 305/305 [00:03&lt;00:00, 149.37it/s]"
          }
        },
        "70ffd13684be4cea9306fadaebb743f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73fec9bd6c4645a5a78bd4d27cc171f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05f7c31612a341018fb0812105d2b37c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "831622595932480fba9d6e7a58dce41b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "670f5c889f9748c7b2a37cfecc674022": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "716875445e814266aa06d408d6e7edc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ac4ce33a1f74874997ec89cd0723b16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}